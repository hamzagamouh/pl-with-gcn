Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_0_
Layers :  [1024, 32, 64, 128, 2]
Dropout rate :  0
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.579, train mcc : 0.160 , val mcc : 0.177
In epoch 50, loss: 0.471, train mcc : 0.385 , val mcc : 0.264
In epoch 100, loss: 0.446, train mcc : 0.420 , val mcc : 0.273
In epoch 150, loss: 0.453, train mcc : 0.442 , val mcc : 0.265
In epoch 200, loss: 0.481, train mcc : 0.452 , val mcc : 0.247
In epoch 250, loss: 0.452, train mcc : 0.465 , val mcc : 0.272
In epoch 300, loss: 0.485, train mcc : 0.475 , val mcc : 0.258
In epoch 350, loss: 0.416, train mcc : 0.482 , val mcc : 0.251
In epoch 400, loss: 0.450, train mcc : 0.487 , val mcc : 0.269
In epoch 450, loss: 0.389, train mcc : 0.491 , val mcc : 0.275
In epoch 500, loss: 0.467, train mcc : 0.498 , val mcc : 0.272
In epoch 550, loss: 0.402, train mcc : 0.499 , val mcc : 0.259
In epoch 600, loss: 0.442, train mcc : 0.509 , val mcc : 0.255
In epoch 650, loss: 0.527, train mcc : 0.513 , val mcc : 0.249
In epoch 700, loss: 0.374, train mcc : 0.518 , val mcc : 0.253
In epoch 750, loss: 0.393, train mcc : 0.511 , val mcc : 0.282
In epoch 800, loss: 0.425, train mcc : 0.525 , val mcc : 0.251
In epoch 850, loss: 0.383, train mcc : 0.525 , val mcc : 0.258
In epoch 900, loss: 0.428, train mcc : 0.521 , val mcc : 0.246
In epoch 950, loss: 0.424, train mcc : 0.526 , val mcc : 0.265
In epoch 1000, loss: 0.424, train mcc : 0.533 , val mcc : 0.247
