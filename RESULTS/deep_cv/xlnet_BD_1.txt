Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_1_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0.3
Using Batch Normalization
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.567, train mcc : 0.184 , val mcc : 0.151
In epoch 50, loss: 0.460, train mcc : 0.472 , val mcc : 0.269
In epoch 100, loss: 0.430, train mcc : 0.520 , val mcc : 0.281
In epoch 150, loss: 0.401, train mcc : 0.544 , val mcc : 0.289
In epoch 200, loss: 0.396, train mcc : 0.558 , val mcc : 0.296
In epoch 250, loss: 0.403, train mcc : 0.569 , val mcc : 0.304
In epoch 300, loss: 0.382, train mcc : 0.576 , val mcc : 0.292
In epoch 350, loss: 0.369, train mcc : 0.583 , val mcc : 0.302
In epoch 400, loss: 0.422, train mcc : 0.591 , val mcc : 0.302
In epoch 450, loss: 0.379, train mcc : 0.599 , val mcc : 0.297
In epoch 500, loss: 0.393, train mcc : 0.600 , val mcc : 0.295
In epoch 550, loss: 0.416, train mcc : 0.605 , val mcc : 0.314
In epoch 600, loss: 0.395, train mcc : 0.609 , val mcc : 0.310
In epoch 650, loss: 0.410, train mcc : 0.613 , val mcc : 0.302
In epoch 700, loss: 0.411, train mcc : 0.618 , val mcc : 0.306
In epoch 750, loss: 0.387, train mcc : 0.617 , val mcc : 0.315
In epoch 800, loss: 0.377, train mcc : 0.620 , val mcc : 0.307
In epoch 850, loss: 0.404, train mcc : 0.624 , val mcc : 0.303
In epoch 900, loss: 0.405, train mcc : 0.625 , val mcc : 0.305
In epoch 950, loss: 0.371, train mcc : 0.628 , val mcc : 0.307
In epoch 1000, loss: 0.367, train mcc : 0.625 , val mcc : 0.301
