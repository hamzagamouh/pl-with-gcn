Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_4_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.614, train mcc : 0.165 , val mcc : 0.214
In epoch 50, loss: 0.474, train mcc : 0.421 , val mcc : 0.292
In epoch 100, loss: 0.425, train mcc : 0.453 , val mcc : 0.293
In epoch 150, loss: 0.432, train mcc : 0.470 , val mcc : 0.288
In epoch 200, loss: 0.437, train mcc : 0.476 , val mcc : 0.284
In epoch 250, loss: 0.422, train mcc : 0.493 , val mcc : 0.293
In epoch 300, loss: 0.448, train mcc : 0.504 , val mcc : 0.293
In epoch 350, loss: 0.420, train mcc : 0.499 , val mcc : 0.283
In epoch 400, loss: 0.457, train mcc : 0.523 , val mcc : 0.294
In epoch 450, loss: 0.422, train mcc : 0.523 , val mcc : 0.287
In epoch 500, loss: 0.388, train mcc : 0.513 , val mcc : 0.313
In epoch 550, loss: 0.407, train mcc : 0.518 , val mcc : 0.304
In epoch 600, loss: 0.435, train mcc : 0.529 , val mcc : 0.298
In epoch 650, loss: 0.420, train mcc : 0.536 , val mcc : 0.302
In epoch 700, loss: 0.407, train mcc : 0.550 , val mcc : 0.304
In epoch 750, loss: 0.437, train mcc : 0.550 , val mcc : 0.297
In epoch 800, loss: 0.404, train mcc : 0.540 , val mcc : 0.303
In epoch 850, loss: 0.380, train mcc : 0.542 , val mcc : 0.299
In epoch 900, loss: 0.466, train mcc : 0.542 , val mcc : 0.296
In epoch 950, loss: 0.402, train mcc : 0.570 , val mcc : 0.302
In epoch 1000, loss: 0.429, train mcc : 0.556 , val mcc : 0.293
