Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_0_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0.3
Using Batch Normalization
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.530, train mcc : 0.188 , val mcc : 0.188
In epoch 50, loss: 0.469, train mcc : 0.485 , val mcc : 0.283
In epoch 100, loss: 0.429, train mcc : 0.521 , val mcc : 0.289
In epoch 150, loss: 0.411, train mcc : 0.547 , val mcc : 0.283
In epoch 200, loss: 0.392, train mcc : 0.564 , val mcc : 0.294
In epoch 250, loss: 0.422, train mcc : 0.574 , val mcc : 0.298
In epoch 300, loss: 0.403, train mcc : 0.581 , val mcc : 0.297
In epoch 350, loss: 0.392, train mcc : 0.584 , val mcc : 0.300
In epoch 400, loss: 0.407, train mcc : 0.590 , val mcc : 0.294
In epoch 450, loss: 0.399, train mcc : 0.599 , val mcc : 0.291
In epoch 500, loss: 0.405, train mcc : 0.605 , val mcc : 0.302
In epoch 550, loss: 0.391, train mcc : 0.604 , val mcc : 0.308
In epoch 600, loss: 0.385, train mcc : 0.608 , val mcc : 0.304
In epoch 650, loss: 0.366, train mcc : 0.611 , val mcc : 0.301
In epoch 700, loss: 0.410, train mcc : 0.616 , val mcc : 0.310
In epoch 750, loss: 0.363, train mcc : 0.617 , val mcc : 0.303
In epoch 800, loss: 0.418, train mcc : 0.622 , val mcc : 0.302
In epoch 850, loss: 0.365, train mcc : 0.624 , val mcc : 0.306
In epoch 900, loss: 0.360, train mcc : 0.627 , val mcc : 0.308
In epoch 950, loss: 0.372, train mcc : 0.629 , val mcc : 0.304
In epoch 1000, loss: 0.408, train mcc : 0.625 , val mcc : 0.309
