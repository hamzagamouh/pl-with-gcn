Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_2_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.584, train mcc : 0.167 , val mcc : 0.214
In epoch 50, loss: 0.508, train mcc : 0.419 , val mcc : 0.290
In epoch 100, loss: 0.448, train mcc : 0.432 , val mcc : 0.291
In epoch 150, loss: 0.479, train mcc : 0.460 , val mcc : 0.242
In epoch 200, loss: 0.441, train mcc : 0.482 , val mcc : 0.276
In epoch 250, loss: 0.427, train mcc : 0.483 , val mcc : 0.272
In epoch 300, loss: 0.443, train mcc : 0.508 , val mcc : 0.273
In epoch 350, loss: 0.386, train mcc : 0.498 , val mcc : 0.271
In epoch 400, loss: 0.404, train mcc : 0.519 , val mcc : 0.282
In epoch 450, loss: 0.461, train mcc : 0.535 , val mcc : 0.273
In epoch 500, loss: 0.388, train mcc : 0.531 , val mcc : 0.273
In epoch 550, loss: 0.390, train mcc : 0.543 , val mcc : 0.282
In epoch 600, loss: 0.431, train mcc : 0.555 , val mcc : 0.299
In epoch 650, loss: 0.390, train mcc : 0.550 , val mcc : 0.289
In epoch 700, loss: 0.413, train mcc : 0.567 , val mcc : 0.289
In epoch 750, loss: 0.419, train mcc : 0.569 , val mcc : 0.297
In epoch 800, loss: 0.405, train mcc : 0.576 , val mcc : 0.301
In epoch 850, loss: 0.416, train mcc : 0.574 , val mcc : 0.305
In epoch 900, loss: 0.445, train mcc : 0.573 , val mcc : 0.290
In epoch 950, loss: 0.426, train mcc : 0.548 , val mcc : 0.288
In epoch 1000, loss: 0.479, train mcc : 0.568 , val mcc : 0.293
