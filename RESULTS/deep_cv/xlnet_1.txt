Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_1_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.524, train mcc : 0.165 , val mcc : 0.206
In epoch 50, loss: 0.475, train mcc : 0.422 , val mcc : 0.256
In epoch 100, loss: 0.503, train mcc : 0.441 , val mcc : 0.279
In epoch 150, loss: 0.442, train mcc : 0.464 , val mcc : 0.287
In epoch 200, loss: 0.420, train mcc : 0.472 , val mcc : 0.283
In epoch 250, loss: 0.400, train mcc : 0.488 , val mcc : 0.283
In epoch 300, loss: 0.452, train mcc : 0.500 , val mcc : 0.254
In epoch 350, loss: 0.450, train mcc : 0.503 , val mcc : 0.294
In epoch 400, loss: 0.426, train mcc : 0.511 , val mcc : 0.293
In epoch 450, loss: 0.479, train mcc : 0.515 , val mcc : 0.290
In epoch 500, loss: 0.474, train mcc : 0.508 , val mcc : 0.274
In epoch 550, loss: 0.423, train mcc : 0.520 , val mcc : 0.296
In epoch 600, loss: 0.416, train mcc : 0.543 , val mcc : 0.304
In epoch 650, loss: 0.479, train mcc : 0.542 , val mcc : 0.258
In epoch 700, loss: 0.438, train mcc : 0.545 , val mcc : 0.303
In epoch 750, loss: 0.393, train mcc : 0.559 , val mcc : 0.293
In epoch 800, loss: 0.440, train mcc : 0.566 , val mcc : 0.316
In epoch 850, loss: 0.413, train mcc : 0.550 , val mcc : 0.294
In epoch 900, loss: 0.413, train mcc : 0.545 , val mcc : 0.297
In epoch 950, loss: 0.400, train mcc : 0.583 , val mcc : 0.311
In epoch 1000, loss: 0.425, train mcc : 0.579 , val mcc : 0.292
