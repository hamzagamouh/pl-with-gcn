Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_2_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0.3
Using Batch Normalization
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.573, train mcc : 0.189 , val mcc : 0.183
In epoch 50, loss: 0.403, train mcc : 0.477 , val mcc : 0.275
In epoch 100, loss: 0.431, train mcc : 0.521 , val mcc : 0.293
In epoch 150, loss: 0.417, train mcc : 0.539 , val mcc : 0.295
In epoch 200, loss: 0.426, train mcc : 0.561 , val mcc : 0.304
In epoch 250, loss: 0.408, train mcc : 0.569 , val mcc : 0.308
In epoch 300, loss: 0.410, train mcc : 0.580 , val mcc : 0.307
In epoch 350, loss: 0.397, train mcc : 0.580 , val mcc : 0.305
In epoch 400, loss: 0.360, train mcc : 0.593 , val mcc : 0.301
In epoch 450, loss: 0.385, train mcc : 0.598 , val mcc : 0.314
In epoch 500, loss: 0.398, train mcc : 0.602 , val mcc : 0.310
In epoch 550, loss: 0.373, train mcc : 0.606 , val mcc : 0.308
In epoch 600, loss: 0.385, train mcc : 0.610 , val mcc : 0.300
In epoch 650, loss: 0.371, train mcc : 0.613 , val mcc : 0.312
In epoch 700, loss: 0.395, train mcc : 0.615 , val mcc : 0.315
In epoch 750, loss: 0.384, train mcc : 0.617 , val mcc : 0.312
In epoch 800, loss: 0.368, train mcc : 0.619 , val mcc : 0.311
In epoch 850, loss: 0.370, train mcc : 0.621 , val mcc : 0.305
In epoch 900, loss: 0.379, train mcc : 0.623 , val mcc : 0.314
In epoch 950, loss: 0.369, train mcc : 0.626 , val mcc : 0.314
In epoch 1000, loss: 0.372, train mcc : 0.631 , val mcc : 0.317
