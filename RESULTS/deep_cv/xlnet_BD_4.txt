Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_4_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0.3
Using Batch Normalization
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.590, train mcc : 0.184 , val mcc : 0.192
In epoch 50, loss: 0.417, train mcc : 0.483 , val mcc : 0.279
In epoch 100, loss: 0.452, train mcc : 0.519 , val mcc : 0.287
In epoch 150, loss: 0.420, train mcc : 0.541 , val mcc : 0.288
In epoch 200, loss: 0.386, train mcc : 0.562 , val mcc : 0.293
In epoch 250, loss: 0.385, train mcc : 0.570 , val mcc : 0.302
In epoch 300, loss: 0.416, train mcc : 0.579 , val mcc : 0.300
In epoch 350, loss: 0.443, train mcc : 0.580 , val mcc : 0.296
In epoch 400, loss: 0.399, train mcc : 0.591 , val mcc : 0.298
In epoch 450, loss: 0.388, train mcc : 0.599 , val mcc : 0.301
In epoch 500, loss: 0.419, train mcc : 0.601 , val mcc : 0.306
In epoch 550, loss: 0.419, train mcc : 0.606 , val mcc : 0.307
In epoch 600, loss: 0.410, train mcc : 0.607 , val mcc : 0.301
In epoch 650, loss: 0.387, train mcc : 0.610 , val mcc : 0.311
In epoch 700, loss: 0.354, train mcc : 0.613 , val mcc : 0.310
In epoch 750, loss: 0.380, train mcc : 0.617 , val mcc : 0.308
In epoch 800, loss: 0.391, train mcc : 0.609 , val mcc : 0.303
In epoch 850, loss: 0.385, train mcc : 0.623 , val mcc : 0.301
In epoch 900, loss: 0.385, train mcc : 0.624 , val mcc : 0.307
In epoch 950, loss: 0.383, train mcc : 0.627 , val mcc : 0.300
In epoch 1000, loss: 0.341, train mcc : 0.627 , val mcc : 0.303
