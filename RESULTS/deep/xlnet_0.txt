Using backend: pytorch
HELLO !
CUDA is available !!
BEGIN TRAINING 
Embedding :  xlnet
CV Fold : fold_0_
Layers :  [1024, 512, 512, 512, 512, 512, 512, 2]
Dropout rate :  0
Training for 1000 Epochs 
Batch size :  32
In epoch 0, loss: 0.552, train mcc : 0.174 , val mcc : 0.206
In epoch 50, loss: 0.489, train mcc : 0.431 , val mcc : 0.276
In epoch 100, loss: 0.467, train mcc : 0.458 , val mcc : 0.259
In epoch 150, loss: 0.486, train mcc : 0.470 , val mcc : 0.239
In epoch 200, loss: 0.422, train mcc : 0.481 , val mcc : 0.267
In epoch 250, loss: 0.421, train mcc : 0.474 , val mcc : 0.251
In epoch 300, loss: 0.412, train mcc : 0.507 , val mcc : 0.277
In epoch 350, loss: 0.437, train mcc : 0.525 , val mcc : 0.277
In epoch 400, loss: 0.409, train mcc : 0.523 , val mcc : 0.270
In epoch 450, loss: 0.441, train mcc : 0.530 , val mcc : 0.281
In epoch 500, loss: 0.426, train mcc : 0.534 , val mcc : 0.276
In epoch 550, loss: 0.425, train mcc : 0.541 , val mcc : 0.290
In epoch 600, loss: 0.396, train mcc : 0.527 , val mcc : 0.293
In epoch 650, loss: 0.405, train mcc : 0.559 , val mcc : 0.272
In epoch 700, loss: 0.468, train mcc : 0.540 , val mcc : 0.263
In epoch 750, loss: 0.447, train mcc : 0.569 , val mcc : 0.301
In epoch 800, loss: 0.442, train mcc : 0.571 , val mcc : 0.293
In epoch 850, loss: 0.381, train mcc : 0.577 , val mcc : 0.303
In epoch 900, loss: 0.389, train mcc : 0.583 , val mcc : 0.292
In epoch 950, loss: 0.424, train mcc : 0.573 , val mcc : 0.291
In epoch 1000, loss: 0.394, train mcc : 0.584 , val mcc : 0.308
